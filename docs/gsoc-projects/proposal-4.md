---
title: "Pipeline for Extraction, Structuring, and Ingestion of Unstructured Data into a Governmental Datalakehouse"
order: 4
size: "Large (350 hours)"
difficulty: "Difficult"
external_link: "https://gov-hub.io/"
external_label: "Visit GovHub"
outcomes:
  - "Functional pipeline for ingesting and parsing unstructured documents."
  - "Structured data persisted in a datalakehouse with metadata and traceability."
  - "Complete technical documentation, quality metrics and reprocessing guidelines."
required_skills:
  - "Languages: Python"
  - "Frameworks/Stacks: OCR libraries, document processing pipelines, LLM APIs"
  - "Knowledge: Unstructured data processing, NLP, datalakehouse architectures"
nice_to_have:
  - "Experience with document layout analysis."
  - "Knowledge of analytical data modeling."
  - "Familiarity with datalakehouse architectures."
  - "Docker and observability practices."
mentors:
  - "Mateus de Castro (@mat054)"
  - "Davi de Aguiar (@davi-aguiar-vieira)"
description: >
  A significant portion of relevant government information remains in unstructured documents such as PDFs,
  presentations and technical reports. Although these documents are public, their content is difficult to reuse
  systematically in analytical pipelines.

  This project proposes a modular pipeline to ingest these documents into GovHub, combining layout analysis,
  region-based OCR and semantic enrichment to produce structured and semi-structured data in well-defined layers
  (raw, parsed, enriched). The final result should be a robust, reprocessable flow that preserves links back to
  the original documents and is ready to support analytical and AI-based applications.
---

A significant portion of relevant government information remains in unstructured documents such as PDFs, presentations and technical reports. Although these documents are public, their content is difficult to reuse systematically in analytical pipelines.

This project proposes a modular pipeline to ingest these documents into GovHub, combining layout analysis, region-based OCR and semantic enrichment to produce structured and semi-structured data in well-defined layers (raw, parsed, enriched). The final result should be a robust, reprocessable flow that preserves links back to the original documents and is ready to support analytical and AI-based applications.

